{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shasuf/PathOr/blob/main/bot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import webbrowser\n",
        "import platform\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "def open_webpage(url):\n",
        "    \"\"\"\n",
        "    Opens a webpage in the default browser.\n",
        "    \"\"\"\n",
        "    webbrowser.open(url)\n",
        "\n",
        "def close_tab():\n",
        "    \"\"\"\n",
        "    Closes the currently active tab in the default browser.\n",
        "    \"\"\"\n",
        "    system = platform.system()\n",
        "    if system == \"Windows\":\n",
        "        subprocess.run(['taskkill', '/F', '/IM', 'chrome.exe'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "    elif system == \"Darwin\":\n",
        "        subprocess.run(['osascript', '-e', 'tell application \"Google Chrome\" to close active tab'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "    else:\n",
        "        print(\"Unsupported platform for closing tabs.\")\n",
        "def conversational_agent():\n",
        "    \"\"\"\n",
        "    Handles user interactions in a conversational manner.\n",
        "    \"\"\"\n",
        "    print(\"ü§ñ Hello! I'm your conversational agent. How can I assist you today?\")\n",
        "    while True:\n",
        "        user_input = input(\"You: \").lower()\n",
        "        if \"open\" in user_input:\n",
        "            if \"google\" in user_input:\n",
        "                open_webpage(\"https://www.google.com\")\n",
        "                print(\"üåê Google opened in your default browser.\")\n",
        "            else:\n",
        "                print(\"I'm sorry, I can only open Google for now.\")\n",
        "        elif \"close\" in user_input and \"tab\" in user_input:\n",
        "            close_tab()\n",
        "            print(\"üö™ Closed the tab in your default browser.\")\n",
        "        elif \"exit\" in user_input or \"quit\" in user_input:\n",
        "            print(\"Goodbye! üëã\")\n",
        "            break\n",
        "        else:\n",
        "            print(\"I'm sorry, I didn't understand that.\")\n",
        "if __name__ == \"__main__\":\n",
        "    conversational_agent()\n"
      ],
      "metadata": {
        "id": "apvzsXpvd5SC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install PyPDF2\n"
      ],
      "metadata": {
        "id": "MTAOvrQjUEJc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2316930d-9cf1-447f-ca3c-9f7a538e7b01"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Os_Z9kwlTNXN",
        "outputId": "dd07a694-727b-4033-e061-2e2305a320e3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-mFwWGfTTLl",
        "outputId": "c9254e83-e666-4a66-96b1-74ba688785bf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"\n",
        "    Extracts text from a PDF file.\n",
        "\n",
        "    Args:\n",
        "    - pdf_path (str): Path to the PDF file.\n",
        "\n",
        "    Returns:\n",
        "    - text (str): Extracted text from the PDF.\n",
        "    \"\"\"\n",
        "    text = \"\"\n",
        "    with open(pdf_path, \"rb\") as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        for page_num in range(len(reader.pages)):\n",
        "            page = reader.pages[page_num]\n",
        "            text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "# Example usage:\n",
        "pdf_text = extract_text_from_pdf(\"/content/sharfa.pdf\")\n",
        "print(pdf_text[:500])  # Print the first 500 characters of the extracted text\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c2LUqalS0rR",
        "outputId": "4005cc2b-29c0-4174-eadb-2fe4d4a2cfd1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dear Hiring Manager, I am writing to express my interest in the Data Scientist  position at, as advertised. With a strong background in developing cutting-edge AI solutions and a track record of driving innovation, I am excited about the opportunity to contribute to your team and help advance the company's mission. In my previous role as a Computer Vision and Machine learning Engineer, I had the privilege of collaborating closely with senior de-velopers and engineers to develop real-time compute\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Preprocesses the text by tokenizing, lowercasing, removing punctuation,\n",
        "    and stop words.\n",
        "\n",
        "    Args:\n",
        "    - text (str): Input text.\n",
        "\n",
        "    Returns:\n",
        "    - preprocessed_text (str): Preprocessed text.\n",
        "    \"\"\"\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Lowercase and remove punctuation\n",
        "    tokens = [token.lower() for token in tokens if token not in string.punctuation]\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # Join tokens back into a string\n",
        "    preprocessed_text = ' '.join(tokens)\n",
        "\n",
        "    return preprocessed_text\n",
        "\n",
        "def preprocess_question(question):\n",
        "    \"\"\"\n",
        "    Preprocesses the question by lowercasing and removing punctuation.\n",
        "\n",
        "    Args:\n",
        "    - question (str): Input question.\n",
        "\n",
        "    Returns:\n",
        "    - preprocessed_question (str): Preprocessed question.\n",
        "    \"\"\"\n",
        "    # Lowercase and remove punctuation\n",
        "    question = re.sub(r'[^\\w\\s]', '', question.lower())\n",
        "    return question\n",
        "\n",
        "# Example usage:\n",
        "preprocessed_pdf_text = preprocess_text(pdf_text)\n",
        "preprocessed_question = preprocess_question(\"What is the main topic of the PDF?\")\n",
        "print(preprocessed_pdf_text[:500])  # Print the first 500 characters of the preprocessed text\n",
        "print(preprocessed_question)  # Print the preprocessed question\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghTeEX15TBjv",
        "outputId": "49b74329-a06c-41d3-c89d-adc203de2a34"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dear hiring manager writing express interest data scientist position advertised strong background developing cutting-edge ai solutions track record driving innovation excited opportunity contribute team help advance company 's mission previous role computer vision machine learning engineer privilege collaborating closely senior de-velopers engineers develop real-time computer vision machine learning solutions using iot devices smart cameras experience honed ability tackle complex challenges deli\n",
            "what is the main topic of the pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import math\n",
        "from collections import Counter\n",
        "\n",
        "def tokenize(text):\n",
        "    \"\"\"\n",
        "    Tokenizes the text by splitting on whitespace and removing punctuation.\n",
        "\n",
        "    Args:\n",
        "    - text (str): Input text.\n",
        "\n",
        "    Returns:\n",
        "    - tokens (list): List of tokens.\n",
        "    \"\"\"\n",
        "    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "    return tokens\n",
        "\n",
        "def compute_tf(tokens):\n",
        "    \"\"\"\n",
        "    Computes the term frequency (TF) for each token in the list of tokens.\n",
        "\n",
        "    Args:\n",
        "    - tokens (list): List of tokens.\n",
        "\n",
        "    Returns:\n",
        "    - tf (dict): Dictionary containing the TF for each token.\n",
        "    \"\"\"\n",
        "    tf = Counter(tokens)\n",
        "    total_tokens = len(tokens)\n",
        "    for token in tf:\n",
        "        tf[token] /= total_tokens\n",
        "    return tf\n",
        "\n",
        "def compute_idf(docs):\n",
        "    \"\"\"\n",
        "    Computes the inverse document frequency (IDF) for each token in the list of documents.\n",
        "\n",
        "    Args:\n",
        "    - docs (list of list): List of documents, where each document is represented as a list of tokens.\n",
        "\n",
        "    Returns:\n",
        "    - idf (dict): Dictionary containing the IDF for each token.\n",
        "    \"\"\"\n",
        "    idf = {}\n",
        "    num_docs = len(docs)\n",
        "    for doc in docs:\n",
        "        doc_set = set(doc)\n",
        "        for token in doc_set:\n",
        "            idf[token] = idf.get(token, 0) + 1\n",
        "    for token, freq in idf.items():\n",
        "        idf[token] = math.log(num_docs / (freq + 1))\n",
        "    return idf\n",
        "\n",
        "def compute_tfidf(tf, idf):\n",
        "    \"\"\"\n",
        "    Computes the TF-IDF score for each token given the TF and IDF values.\n",
        "\n",
        "    Args:\n",
        "    - tf (dict): Dictionary containing the TF for each token.\n",
        "    - idf (dict): Dictionary containing the IDF for each token.\n",
        "\n",
        "    Returns:\n",
        "    - tfidf (dict): Dictionary containing the TF-IDF score for each token.\n",
        "    \"\"\"\n",
        "    tfidf = {}\n",
        "    for token, tf_value in tf.items():\n",
        "        tfidf[token] = tf_value * idf[token]\n",
        "    return tfidf\n",
        "\n",
        "def preprocess_and_compute_tfidf(documents):\n",
        "    \"\"\"\n",
        "    Preprocesses the documents (PDF text and question) and computes TF-IDF vectors.\n",
        "\n",
        "    Args:\n",
        "    - documents (list of str): List of documents, where each document is represented as a string.\n",
        "\n",
        "    Returns:\n",
        "    - tfidf_vectors (list of dict): List of TF-IDF vectors for each document.\n",
        "    \"\"\"\n",
        "    tokenized_documents = [tokenize(doc) for doc in documents]\n",
        "    tf_documents = [compute_tf(doc) for doc in tokenized_documents]\n",
        "    idf = compute_idf(tokenized_documents)\n",
        "    tfidf_vectors = [compute_tfidf(tf_doc, idf) for tf_doc in tf_documents]\n",
        "    return tfidf_vectors\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ktCsDBzRTBor"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "pdf_text = \"Dear Hiring Manager, I am writing to express my interest in the Data Scientist position...\"\n",
        "question = \"What is the main topic of the PDF?\"\n",
        "documents = [pdf_text, question]\n",
        "tfidf_vectors = preprocess_and_compute_tfidf(documents)\n",
        "\n",
        "# Print the TF-IDF vectors\n",
        "print(tfidf_vectors)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZ7kzmivS0tj",
        "outputId": "cfa726af-9fe8-4da5-c367-fe66283cf825"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'dear': 0.0, 'hiring': 0.0, 'manager': 0.0, 'i': 0.0, 'am': 0.0, 'writing': 0.0, 'to': 0.0, 'express': 0.0, 'my': 0.0, 'interest': 0.0, 'in': 0.0, 'the': -0.027031007207210963, 'data': 0.0, 'scientist': 0.0, 'position': 0.0}, {'what': 0.0, 'is': 0.0, 'the': -0.10136627702704111, 'main': 0.0, 'topic': 0.0, 'of': 0.0, 'pdf': 0.0}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import math\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "def tokenize(text):\n",
        "    \"\"\"\n",
        "    Tokenizes the text by splitting on whitespace and removing punctuation.\n",
        "\n",
        "    Args:\n",
        "    - text (str): Input text.\n",
        "\n",
        "    Returns:\n",
        "    - tokens (list): List of tokens.\n",
        "    \"\"\"\n",
        "    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "    return tokens\n",
        "\n",
        "def compute_tf(tokens):\n",
        "    \"\"\"\n",
        "    Computes the term frequency (TF) for each token in the list of tokens.\n",
        "\n",
        "    Args:\n",
        "    - tokens (list): List of tokens.\n",
        "\n",
        "    Returns:\n",
        "    - tf (dict): Dictionary containing the TF for each token.\n",
        "    \"\"\"\n",
        "    tf = Counter(tokens)\n",
        "    total_tokens = len(tokens)\n",
        "    for token in tf:\n",
        "        tf[token] /= total_tokens\n",
        "    return tf\n",
        "\n",
        "def compute_idf(docs):\n",
        "    \"\"\"\n",
        "    Computes the inverse document frequency (IDF) for each token in the list of documents.\n",
        "\n",
        "    Args:\n",
        "    - docs (list of list): List of documents, where each document is represented as a list of tokens.\n",
        "\n",
        "    Returns:\n",
        "    - idf (dict): Dictionary containing the IDF for each token.\n",
        "    \"\"\"\n",
        "    idf = {}\n",
        "    num_docs = len(docs)\n",
        "    for doc in docs:\n",
        "        doc_set = set(doc)\n",
        "        for token in doc_set:\n",
        "            idf[token] = idf.get(token, 0) + 1\n",
        "    for token, freq in idf.items():\n",
        "        idf[token] = math.log(num_docs / (freq + 1))\n",
        "    return idf\n",
        "\n",
        "def compute_idf(docs):\n",
        "    \"\"\"\n",
        "    Computes the inverse document frequency (IDF) for each token in the list of documents.\n",
        "\n",
        "    Args:\n",
        "    - docs (list of list): List of documents, where each document is represented as a list of tokens.\n",
        "\n",
        "    Returns:\n",
        "    - idf (dict): Dictionary containing the IDF for each token.\n",
        "    \"\"\"\n",
        "    idf = {}\n",
        "    num_docs = len(docs)\n",
        "    for doc in docs:\n",
        "        doc_set = set(doc)\n",
        "        for token in doc_set:\n",
        "            idf[token] = idf.get(token, 0) + 1\n",
        "    for token, freq in idf.items():\n",
        "        idf[token] = math.log(num_docs / freq)  # Fix the IDF calculation\n",
        "    return idf\n",
        "\n",
        "\n",
        "def preprocess_and_compute_tfidf(documents):\n",
        "    \"\"\"\n",
        "    Preprocesses the documents (PDF text and question) and computes TF-IDF vectors.\n",
        "\n",
        "    Args:\n",
        "    - documents (list of str): List of documents, where each document is represented as a string.\n",
        "\n",
        "    Returns:\n",
        "    - tfidf_vectors (list of dict): List of TF-IDF vectors for the PDF text and the question.\n",
        "    \"\"\"\n",
        "    tokenized_documents = [tokenize(doc) for doc in documents]\n",
        "    tf_documents = [compute_tf(doc) for doc in tokenized_documents]\n",
        "    idf = compute_idf(tokenized_documents)\n",
        "    tfidf_vectors = [compute_tfidf(tf_doc, idf) for tf_doc in tf_documents]\n",
        "\n",
        "    # Ensure tfidf_vectors is in the expected format\n",
        "    assert len(tfidf_vectors) == 2, \"Expected two TF-IDF vectors, but found {}\".format(len(tfidf_vectors))\n",
        "    for i, tfidf_vector in enumerate(tfidf_vectors):\n",
        "        if not isinstance(tfidf_vector, dict):\n",
        "            # If TF-IDF vector is not a dictionary, create an empty one\n",
        "            print(\"Warning: TF-IDF vector {} is not a dictionary. Creating an empty one.\".format(i + 1))\n",
        "            tfidf_vectors[i] = {}\n",
        "\n",
        "    return tfidf_vectors\n",
        "\n",
        "def compute_cosine_similarity(tfidf_vector1, tfidf_vector2):\n",
        "    \"\"\"\n",
        "    Computes the cosine similarity between two TF-IDF vectors.\n",
        "\n",
        "    Args:\n",
        "    - tfidf_vector1 (dict): TF-IDF vector 1.\n",
        "    - tfidf_vector2 (dict): TF-IDF vector 2.\n",
        "\n",
        "    Returns:\n",
        "    - cosine_similarity (float): Cosine similarity between the two TF-IDF vectors.\n",
        "    \"\"\"\n",
        "    if not isinstance(tfidf_vector1, dict) or not isinstance(tfidf_vector2, dict):\n",
        "        print(\"Error: TF-IDF vectors are not properly computed.\")\n",
        "        return None\n",
        "\n",
        "    tokens = set(tfidf_vector1.keys()) | set(tfidf_vector2.keys())\n",
        "    dot_product = sum(tfidf_vector1.get(token, 0) * tfidf_vector2.get(token, 0) for token in tokens)\n",
        "    norm1 = np.linalg.norm(list(tfidf_vector1.values()))\n",
        "    norm2 = np.linalg.norm(list(tfidf_vector2.values()))\n",
        "    cosine_similarity = dot_product / (norm1 * norm2)\n",
        "    return cosine_similarity\n",
        "\n",
        "def retrieve_relevant_passages(tfidf_vectors, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Retrieves relevant passages from the PDF based on cosine similarity with the question.\n",
        "\n",
        "    Args:\n",
        "    - tfidf_vectors (list of dict): List of TF-IDF vectors for the PDF text and the question.\n",
        "    - threshold (float): Similarity threshold for considering a passage as relevant. Default is 0.5.\n",
        "\n",
        "    Returns:\n",
        "    - relevant_passages (list of str): List of relevant passages from the PDF.\n",
        "    \"\"\"\n",
        "    question_tfidf_vector, pdf_tfidf_vector = tfidf_vectors\n",
        "\n",
        "    if question_tfidf_vector is None or pdf_tfidf_vector is None:\n",
        "        print(\"Error: TF-IDF vectors are not properly computed.\")\n",
        "        return []\n",
        "\n",
        "    relevant_passages = []\n",
        "    for passage, passage_tfidf_vector in pdf_tfidf_vector.items():\n",
        "        similarity = compute_cosine_similarity(question_tfidf_vector, passage_tfidf_vector)\n",
        "        if similarity is not None and similarity >= threshold:\n",
        "            relevant_passages.append(passage)\n",
        "    return relevant_passages\n",
        "\n",
        "# Example usage:\n",
        "pdf_text = \"Dear Hiring Manager, I am writing to express my interest in the Data Scientist position...\"\n",
        "question = \"What is the main topic of the PDF?\"\n",
        "documents = [pdf_text, question]\n",
        "tfidf_vectors = preprocess_and_compute_tfidf(documents)\n",
        "print(\"TF-IDF Vectors:\", tfidf_vectors)  # Add this line to inspect tfidf_vectors\n",
        "relevant_passages = retrieve_relevant_passages(tfidf_vectors)\n",
        "print(\"Relevant Passages:\", relevant_passages)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdl3EWt3V6n7",
        "outputId": "1d293201-aa48-4743-90fd-c98e827485cb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Vectors: [{'dear': 0.046209812037329684, 'hiring': 0.046209812037329684, 'manager': 0.046209812037329684, 'i': 0.046209812037329684, 'am': 0.046209812037329684, 'writing': 0.046209812037329684, 'to': 0.046209812037329684, 'express': 0.046209812037329684, 'my': 0.046209812037329684, 'interest': 0.046209812037329684, 'in': 0.046209812037329684, 'the': 0.0, 'data': 0.046209812037329684, 'scientist': 0.046209812037329684, 'position': 0.046209812037329684}, {'what': 0.08664339756999316, 'is': 0.08664339756999316, 'the': 0.0, 'main': 0.08664339756999316, 'topic': 0.08664339756999316, 'of': 0.08664339756999316, 'pdf': 0.08664339756999316}]\n",
            "Error: TF-IDF vectors are not properly computed.\n",
            "Error: TF-IDF vectors are not properly computed.\n",
            "Error: TF-IDF vectors are not properly computed.\n",
            "Error: TF-IDF vectors are not properly computed.\n",
            "Error: TF-IDF vectors are not properly computed.\n",
            "Error: TF-IDF vectors are not properly computed.\n",
            "Error: TF-IDF vectors are not properly computed.\n",
            "Relevant Passages: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def tokenize(text):\n",
        "    \"\"\"\n",
        "    Tokenizes the text by splitting on whitespace and removing punctuation.\n",
        "\n",
        "    Args:\n",
        "    - text (str): Input text.\n",
        "\n",
        "    Returns:\n",
        "    - tokens (list): List of tokens.\n",
        "    \"\"\"\n",
        "    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "    print(\"Tokens:\", tokens)\n",
        "    return tokens\n"
      ],
      "metadata": {
        "id": "IhGh7ZEKYq3T"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "def compute_tf(tokens):\n",
        "    \"\"\"\n",
        "    Computes the term frequency (TF) for each token in the list of tokens.\n",
        "\n",
        "    Args:\n",
        "    - tokens (list): List of tokens.\n",
        "\n",
        "    Returns:\n",
        "    - tf (dict): Dictionary containing the TF for each token.\n",
        "    \"\"\"\n",
        "    tf = Counter(tokens)\n",
        "    total_tokens = len(tokens)\n",
        "    for token in tf:\n",
        "        tf[token] /= total_tokens\n",
        "    print(\"TF:\", tf)\n",
        "    return tf\n"
      ],
      "metadata": {
        "id": "q-NePhz3Yq5Y"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def compute_idf(docs):\n",
        "    \"\"\"\n",
        "    Computes the inverse document frequency (IDF) for each token in the list of documents.\n",
        "\n",
        "    Args:\n",
        "    - docs (list of list): List of documents, where each document is represented as a list of tokens.\n",
        "\n",
        "    Returns:\n",
        "    - idf (dict): Dictionary containing the IDF for each token.\n",
        "    \"\"\"\n",
        "    idf = {}\n",
        "    num_docs = len(docs)\n",
        "    for doc in docs:\n",
        "        doc_set = set(doc)\n",
        "        for token in doc_set:\n",
        "            idf[token] = idf.get(token, 0) + 1\n",
        "    for token, freq in idf.items():\n",
        "        idf[token] = math.log(num_docs / (freq + 1))\n",
        "    print(\"IDF:\", idf)\n",
        "    return idf\n"
      ],
      "metadata": {
        "id": "ek7yjsMqYq7u"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def compute_idf(docs):\n",
        "    \"\"\"\n",
        "    Computes the inverse document frequency (IDF) for each token in the list of documents.\n",
        "\n",
        "    Args:\n",
        "    - docs (list of list): List of documents, where each document is represented as a list of tokens.\n",
        "\n",
        "    Returns:\n",
        "    - idf (dict): Dictionary containing the IDF for each token.\n",
        "    \"\"\"\n",
        "    idf = {}\n",
        "    num_docs = len(docs)\n",
        "    for doc in docs:\n",
        "        doc_set = set(doc)\n",
        "        for token in doc_set:\n",
        "            idf[token] = idf.get(token, 0) + 1\n",
        "    for token, freq in idf.items():\n",
        "        idf[token] = math.log(num_docs / (freq + 1))\n",
        "    print(\"IDF:\", idf)\n",
        "    return idf\n"
      ],
      "metadata": {
        "id": "lfhY04roZhaM"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_tfidf(tf, idf):\n",
        "    \"\"\"\n",
        "    Computes the TF-IDF score for each token given the TF and IDF values.\n",
        "\n",
        "    Args:\n",
        "    - tf (dict): Dictionary containing the TF for each token.\n",
        "    - idf (dict): Dictionary containing the IDF for each token.\n",
        "\n",
        "    Returns:\n",
        "    - tfidf (dict): Dictionary containing the TF-IDF score for each token.\n",
        "    \"\"\"\n",
        "    tfidf = {token: tf[token] * idf[token] for token in tf}\n",
        "    print(\"TF-IDF:\", tfidf)\n",
        "    return tfidf\n"
      ],
      "metadata": {
        "id": "3ycwNBLTZhcP"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_and_compute_tfidf(documents):\n",
        "    \"\"\"\n",
        "    Preprocesses the documents (PDF text and question) and computes TF-IDF vectors.\n",
        "\n",
        "    Args:\n",
        "    - documents (list of str): List of documents, where each document is represented as a string.\n",
        "\n",
        "    Returns:\n",
        "    - tfidf_vectors (list of dict): List of TF-IDF vectors for the PDF text and the question.\n",
        "    \"\"\"\n",
        "    tokenized_documents = [tokenize(doc) for doc in documents]\n",
        "    tf_documents = [compute_tf(doc) for doc in tokenized_documents]\n",
        "    idf = compute_idf(tokenized_documents)\n",
        "    tfidf_vectors = [compute_tfidf(tf_doc, idf) for tf_doc in tf_documents]\n",
        "\n",
        "    # Ensure tfidf_vectors is in the expected format\n",
        "    assert len(tfidf_vectors) == 2, \"Expected two TF-IDF vectors, but found {}\".format(len(tfidf_vectors))\n",
        "    for i, tfidf_vector in enumerate(tfidf_vectors):\n",
        "        if not isinstance(tfidf_vector, dict):\n",
        "            # If TF-IDF vector is not a dictionary, create an empty one\n",
        "            print(\"Warning: TF-IDF vector {} is not a dictionary. Creating an empty one.\".format(i + 1))\n",
        "            tfidf_vectors[i] = {}\n",
        "\n",
        "    return tfidf_vectors\n"
      ],
      "metadata": {
        "id": "OF0PgLt_ZheU"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_text = \"Dear Hiring Manager, I am writing to express my interest in the Data Scientist position...\"\n",
        "question = \"What is the main topic of the PDF?\"\n",
        "documents = [pdf_text, question]\n",
        "tfidf_vectors = preprocess_and_compute_tfidf(documents)\n",
        "print(\"TF-IDF Vectors:\", tfidf_vectors)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Te38CWCOZhga",
        "outputId": "1d086a74-0c68-4ad0-d564-6ff186e0ff39"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['dear', 'hiring', 'manager', 'i', 'am', 'writing', 'to', 'express', 'my', 'interest', 'in', 'the', 'data', 'scientist', 'position']\n",
            "Tokens: ['what', 'is', 'the', 'main', 'topic', 'of', 'the', 'pdf']\n",
            "TF: Counter({'dear': 0.06666666666666667, 'hiring': 0.06666666666666667, 'manager': 0.06666666666666667, 'i': 0.06666666666666667, 'am': 0.06666666666666667, 'writing': 0.06666666666666667, 'to': 0.06666666666666667, 'express': 0.06666666666666667, 'my': 0.06666666666666667, 'interest': 0.06666666666666667, 'in': 0.06666666666666667, 'the': 0.06666666666666667, 'data': 0.06666666666666667, 'scientist': 0.06666666666666667, 'position': 0.06666666666666667})\n",
            "TF: Counter({'the': 0.25, 'what': 0.125, 'is': 0.125, 'main': 0.125, 'topic': 0.125, 'of': 0.125, 'pdf': 0.125})\n",
            "IDF: {'writing': 0.0, 'i': 0.0, 'am': 0.0, 'dear': 0.0, 'express': 0.0, 'position': 0.0, 'manager': 0.0, 'data': 0.0, 'the': -0.40546510810816444, 'to': 0.0, 'interest': 0.0, 'my': 0.0, 'scientist': 0.0, 'hiring': 0.0, 'in': 0.0, 'what': 0.0, 'main': 0.0, 'pdf': 0.0, 'topic': 0.0, 'of': 0.0, 'is': 0.0}\n",
            "TF-IDF: {'dear': 0.0, 'hiring': 0.0, 'manager': 0.0, 'i': 0.0, 'am': 0.0, 'writing': 0.0, 'to': 0.0, 'express': 0.0, 'my': 0.0, 'interest': 0.0, 'in': 0.0, 'the': -0.027031007207210963, 'data': 0.0, 'scientist': 0.0, 'position': 0.0}\n",
            "TF-IDF: {'what': 0.0, 'is': 0.0, 'the': -0.10136627702704111, 'main': 0.0, 'topic': 0.0, 'of': 0.0, 'pdf': 0.0}\n",
            "TF-IDF Vectors: [{'dear': 0.0, 'hiring': 0.0, 'manager': 0.0, 'i': 0.0, 'am': 0.0, 'writing': 0.0, 'to': 0.0, 'express': 0.0, 'my': 0.0, 'interest': 0.0, 'in': 0.0, 'the': -0.027031007207210963, 'data': 0.0, 'scientist': 0.0, 'position': 0.0}, {'what': 0.0, 'is': 0.0, 'the': -0.10136627702704111, 'main': 0.0, 'topic': 0.0, 'of': 0.0, 'pdf': 0.0}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install fitz"
      ],
      "metadata": {
        "id": "VVJzYx-WacTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install frontend"
      ],
      "metadata": {
        "id": "LqamredXa728"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Actual code after all testing\n"
      ],
      "metadata": {
        "id": "oPp8LSaCemzl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import math\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from PyPDF2 import PdfReader\n",
        "\n",
        "def extract_text_from_pdf(pdf_file):\n",
        "    \"\"\"\n",
        "    Extracts text from a PDF file.\n",
        "\n",
        "    Args:\n",
        "    - pdf_file (str): Path to the PDF file.\n",
        "\n",
        "    Returns:\n",
        "    - text (str): Extracted text from the PDF.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(pdf_file, \"rb\") as file:\n",
        "            reader = PdfReader(file)\n",
        "            text = \"\"\n",
        "            for page in reader.pages:\n",
        "                text += page.extract_text()\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(\"An error occurred:\", e)\n",
        "        return None\n",
        "\n",
        "def tokenize(text):\n",
        "    \"\"\"\n",
        "    Tokenizes the text by splitting on whitespace and removing punctuation.\n",
        "\n",
        "    Args:\n",
        "    - text (str): Input text.\n",
        "\n",
        "    Returns:\n",
        "    - tokens (list): List of tokens.\n",
        "    \"\"\"\n",
        "    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "    return tokens\n",
        "\n",
        "def compute_tf(tokens):\n",
        "    \"\"\"\n",
        "    Computes the term frequency (TF) for each token in the list of tokens.\n",
        "\n",
        "    Args:\n",
        "    - tokens (list): List of tokens.\n",
        "\n",
        "    Returns:\n",
        "    - tf (dict): Dictionary containing the TF for each token.\n",
        "    \"\"\"\n",
        "    tf = Counter(tokens)\n",
        "    total_tokens = len(tokens)\n",
        "    for token in tf:\n",
        "        tf[token] /= total_tokens\n",
        "    return tf\n",
        "\n",
        "def compute_idf(docs):\n",
        "    \"\"\"\n",
        "    Computes the inverse document frequency (IDF) for each token in the list of documents.\n",
        "\n",
        "    Args:\n",
        "    - docs (list of list): List of documents, where each document is represented as a list of tokens.\n",
        "\n",
        "    Returns:\n",
        "    - idf (dict): Dictionary containing the IDF for each token.\n",
        "    \"\"\"\n",
        "    idf = {}\n",
        "    num_docs = len(docs)\n",
        "    for doc in docs:\n",
        "        doc_set = set(doc)\n",
        "        for token in doc_set:\n",
        "            idf[token] = idf.get(token, 0) + 1\n",
        "    for token, freq in idf.items():\n",
        "        idf[token] = math.log(num_docs / freq)  # Divide by total number of documents\n",
        "    return idf\n",
        "\n",
        "\n",
        "def compute_tfidf(tf, idf):\n",
        "    \"\"\"\n",
        "    Computes the TF-IDF score for each token given the TF and IDF values.\n",
        "\n",
        "    Args:\n",
        "    - tf (dict): Dictionary containing the TF for each token.\n",
        "    - idf (dict): Dictionary containing the IDF for each token.\n",
        "\n",
        "    Returns:\n",
        "    - tfidf (dict): Dictionary containing the TF-IDF score for each token.\n",
        "    \"\"\"\n",
        "    tfidf = {}\n",
        "    for token, tf_value in tf.items():\n",
        "        tfidf[token] = tf_value * idf[token]\n",
        "    return tfidf\n",
        "\n",
        "def preprocess_and_compute_tfidf(documents):\n",
        "    \"\"\"\n",
        "    Preprocesses the documents (PDF text and question) and computes TF-IDF vectors.\n",
        "\n",
        "    Args:\n",
        "    - documents (list of str): List of documents, where each document is represented as a string.\n",
        "\n",
        "    Returns:\n",
        "    - tfidf_vectors (list of dict): List of TF-IDF vectors for the PDF text and the question.\n",
        "    \"\"\"\n",
        "    tokenized_documents = [tokenize(doc) for doc in documents]\n",
        "    print(\"Tokens:\", tokenized_documents)\n",
        "    tf_documents = [compute_tf(doc) for doc in tokenized_documents]\n",
        "    print(\"TF:\", tf_documents)\n",
        "    idf = compute_idf(tokenized_documents)\n",
        "    print(\"IDF:\", idf)\n",
        "    tfidf_vectors = [compute_tfidf(tf_doc, idf) for tf_doc in tf_documents]\n",
        "    print(\"TF-IDF:\", tfidf_vectors)\n",
        "\n",
        "    # Ensure tfidf_vectors is in the expected format\n",
        "    assert len(tfidf_vectors) == 2, \"Expected two TF-IDF vectors, but found {}\".format(len(tfidf_vectors))\n",
        "    for i, tfidf_vector in enumerate(tfidf_vectors):\n",
        "        if not isinstance(tfidf_vector, dict):\n",
        "            # If TF-IDF vector is not a dictionary, create an empty one\n",
        "            print(\"Warning: TF-IDF vector {} is not a dictionary. Creating an empty one.\".format(i + 1))\n",
        "            tfidf_vectors[i] = {}\n",
        "\n",
        "    return tfidf_vectors\n",
        "\n",
        "def compute_cosine_similarity(tfidf_vector1, tfidf_vector2):\n",
        "    \"\"\"\n",
        "    Computes the cosine similarity between two TF-IDF vectors.\n",
        "\n",
        "    Args:\n",
        "    - tfidf_vector1 (dict): TF-IDF vector 1.\n",
        "    - tfidf_vector2 (dict): TF-IDF vector 2.\n",
        "\n",
        "    Returns:\n",
        "    - cosine_similarity (float): Cosine similarity between the two TF-IDF vectors.\n",
        "    \"\"\"\n",
        "    if not isinstance(tfidf_vector1, dict) or not isinstance(tfidf_vector2, dict):\n",
        "        print(\"Error: TF-IDF vectors are not properly computed.\")\n",
        "        return None\n",
        "\n",
        "    tokens = set(tfidf_vector1.keys()) | set(tfidf_vector2.keys())\n",
        "    dot_product = sum(tfidf_vector1.get(token, 0) * tfidf_vector2.get(token, 0) for token in tokens)\n",
        "    norm1 = np.linalg.norm(list(tfidf_vector1.values()))\n",
        "    norm2 = np.linalg.norm(list(tfidf_vector2.values()))\n",
        "    cosine_similarity = dot_product / (norm1 * norm2)\n",
        "    return cosine_similarity\n",
        "\n",
        "def retrieve_relevant_passages(tfidf_vectors, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Retrieves relevant passages from the PDF based on cosine similarity with the question.\n",
        "\n",
        "    Args:\n",
        "    - tfidf_vectors (list of dict): List of TF-IDF vectors for the PDF text and the question.\n",
        "    - threshold (float): Similarity threshold for considering a passage as relevant. Default is 0.5.\n",
        "\n",
        "    Returns:\n",
        "    - relevant_passages (list of str): List of relevant passages from the PDF.\n",
        "    \"\"\"\n",
        "    question_tfidf_vector, pdf_tfidf_vector = tfidf_vectors\n",
        "\n",
        "    if question_tfidf_vector is None or pdf_tfidf_vector is None:\n",
        "        print(\"Error: TF-IDF vectors are not properly computed.\")\n",
        "        return []\n",
        "\n",
        "    relevant_passages = []\n",
        "    for passage, passage_tfidf_vector in pdf_tfidf_vector.items():\n",
        "        similarity = compute_cosine_similarity(question_tfidf_vector, passage_tfidf_vector)\n",
        "        if similarity is not None and similarity >= threshold:\n",
        "            relevant_passages.append(passage)\n",
        "    return relevant_passages\n",
        "\n",
        "def answer_question(pdf_file, question, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Answers a question based on the content of a PDF file.\n",
        "\n",
        "    Args:\n",
        "    - pdf_file (str): Path to the PDF file.\n",
        "    - question (str): The question to be answered.\n",
        "    - threshold (float): Similarity threshold for considering a passage as relevant. Default is 0.5.\n",
        "\n",
        "    Returns:\n",
        "    - answer (str or None): The answer to the question, or None if no relevant passages are found.\n",
        "    \"\"\"\n",
        "    # Extract text from PDF\n",
        "    pdf_text = extract_text_from_pdf(pdf_file)\n",
        "    if pdf_text is None:\n",
        "        print(\"Error: Failed to extract text from the PDF.\")\n",
        "        return None\n",
        "\n",
        "    # Preprocess PDF text and question, and compute TF-IDF vectors\n",
        "    documents = [pdf_text, question]\n",
        "    tfidf_vectors = preprocess_and_compute_tfidf(documents)\n",
        "    if not tfidf_vectors:\n",
        "        print(\"Error: TF-IDF vectors are not properly computed.\")\n",
        "        return None\n",
        "\n",
        "    # Retrieve relevant passages from the PDF\n",
        "    relevant_passages = retrieve_relevant_passages(tfidf_vectors, threshold)\n",
        "    if not relevant_passages:\n",
        "        print(\"No relevant passages found.\")\n",
        "        return None\n",
        "\n",
        "    # Concatenate relevant passages as the answer\n",
        "    answer = ' '.join(relevant_passages)\n",
        "    return answer\n",
        "\n",
        "# Example usage:\n",
        "pdf_file = \"/content/sharfa.pdf\"\n",
        "question = \"What is qualification?\"\n",
        "answer = answer_question(pdf_file, question)\n",
        "print(\"Answer:\", answer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_-jEzBzblwq",
        "outputId": "613127fe-ff31-431a-c597-91782a90ec04"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: [['dear', 'hiring', 'manager', 'i', 'am', 'writing', 'to', 'express', 'my', 'interest', 'in', 'the', 'data', 'scientist', 'position', 'at', 'as', 'advertised', 'with', 'a', 'strong', 'background', 'in', 'developing', 'cutting', 'edge', 'ai', 'solutions', 'and', 'a', 'track', 'record', 'of', 'driving', 'innovation', 'i', 'am', 'excited', 'about', 'the', 'opportunity', 'to', 'contribute', 'to', 'your', 'team', 'and', 'help', 'advance', 'the', 'company', 's', 'mission', 'in', 'my', 'previous', 'role', 'as', 'a', 'computer', 'vision', 'and', 'machine', 'learning', 'engineer', 'i', 'had', 'the', 'privilege', 'of', 'collaborating', 'closely', 'with', 'senior', 'de', 'velopers', 'and', 'engineers', 'to', 'develop', 'real', 'time', 'computer', 'vision', 'and', 'machine', 'learning', 'solutions', 'using', 'iot', 'devices', 'and', 'smart', 'cameras', 'this', 'experience', 'has', 'honed', 'my', 'ability', 'to', 'tackle', 'complex', 'challenges', 'and', 'deliver', 'scalable', 'solutions', 'that', 'meet', 'both', 'technical', 'and', 'busi', 'ness', 'objectives', 'i', 'have', 'a', 'proven', 'track', 'record', 'of', 'developing', 'machine', 'learning', 'models', 'and', 'algorithms', 'to', 'analyze', 'and', 'interpret', 'data', 'generated', 'by', 'iot', 'de', 'vices', 'and', 'smart', 'cameras', 'additionally', 'i', 'have', 'managed', 'the', 'genera', 'tion', 'of', 'datasets', 'for', 'machine', 'learning', 'model', 'generation', 'evaluated', 'and', 'improved', 'existing', 'machine', 'learning', 'models', 'deployed', 'and', 'col', 'laborated', 'with', 'external', 'teams', 'to', 'integrate', 'iot', 'devices', 'and', 'smart', 'cameras', 'with', 'third', 'party', 'systems', 'my', 'proficiency', 'in', 'python', 'tensorflow', 'pytorch', 'opencv', 'and', 'other', 'relevant', 'technologies', 'has', 'equipped', 'me', 'with', 'the', 'necessary', 'skills', 'to', 'excel', 'in', 'this', 'role', 'i', 'am', 'adept', 'at', 'writing', 'clean', 'well', 'documented', 'and', 'modular', 'code', 'and', 'i', 'have', 'experience', 'experimenting', 'with', 'model', 'ar', 'chitectures', 'performing', 'hyper', 'parameter', 'tuning', 'and', 'conducting', 'transfer', 'learning', 'to', 'optimize', 'model', 'performance', 'furthermore', 'my', 'experience', 'in', 'developing', 'ai', 'models', 'for', 'generating', 'graphics', 'collaborating', 'with', 'cross', 'functional', 'teams', 'and', 'mentoring', 'junior', 'team', 'members', 'aligns', 'closely', 'with', 'the', 'responsibilities', 'outlined', 'in', 'the', 'job', 'description', 'i', 'am', 'passionate', 'about', 'staying', 'abreast', 'of', 'the', 'latest', 'advancements', 'in', 'ai', 'and', 'graphics', 'technologies', 'and', 'am', 'com', 'mitted', 'to', 'driving', 'innovation', 'within', 'your', 'organization', 'i', 'am', 'particularly', 'excited', 'about', 'the', 'opportunity', 'to', 'leverage', 'my', 'ex', 'pertise', 'in', 'machine', 'learning', 'to', 'develop', 'state', 'of', 'the', 'art', 'applications', 'for', 'object', 'detection', 'image', 'classification', 'text', 'classification', 'and', 'other', 'domains', 'i', 'am', 'confident', 'that', 'my', 'background', 'and', 'skill', 'set', 'make', 'me', 'a', 'strong', 'candidate', 'for', 'this', 'position', 'and', 'i', 'am', 'eager', 'to', 'bring', 'my', 'unique', 'perspective', 'and', 'insights', 'to', 'your', 'team', 'warm', 'regards', 'sharfa', 'mallick'], ['what', 'is', 'qualification']]\n",
            "TF: [Counter({'and': 0.06983240223463687, 'to': 0.03910614525139665, 'i': 0.030726256983240222, 'the': 0.030726256983240222, 'in': 0.025139664804469275, 'am': 0.0223463687150838, 'my': 0.0223463687150838, 'with': 0.0223463687150838, 'learning': 0.019553072625698324, 'of': 0.01675977653631285, 'machine': 0.01675977653631285, 'a': 0.013966480446927373, 'for': 0.0111731843575419, 'developing': 0.008379888268156424, 'ai': 0.008379888268156424, 'solutions': 0.008379888268156424, 'about': 0.008379888268156424, 'your': 0.008379888268156424, 'team': 0.008379888268156424, 'iot': 0.008379888268156424, 'smart': 0.008379888268156424, 'cameras': 0.008379888268156424, 'this': 0.008379888268156424, 'experience': 0.008379888268156424, 'have': 0.008379888268156424, 'models': 0.008379888268156424, 'model': 0.008379888268156424, 'writing': 0.00558659217877095, 'data': 0.00558659217877095, 'position': 0.00558659217877095, 'at': 0.00558659217877095, 'as': 0.00558659217877095, 'strong': 0.00558659217877095, 'background': 0.00558659217877095, 'track': 0.00558659217877095, 'record': 0.00558659217877095, 'driving': 0.00558659217877095, 'innovation': 0.00558659217877095, 'excited': 0.00558659217877095, 'opportunity': 0.00558659217877095, 'role': 0.00558659217877095, 'computer': 0.00558659217877095, 'vision': 0.00558659217877095, 'collaborating': 0.00558659217877095, 'closely': 0.00558659217877095, 'de': 0.00558659217877095, 'develop': 0.00558659217877095, 'devices': 0.00558659217877095, 'has': 0.00558659217877095, 'that': 0.00558659217877095, 'teams': 0.00558659217877095, 'other': 0.00558659217877095, 'technologies': 0.00558659217877095, 'me': 0.00558659217877095, 'graphics': 0.00558659217877095, 'classification': 0.00558659217877095, 'dear': 0.002793296089385475, 'hiring': 0.002793296089385475, 'manager': 0.002793296089385475, 'express': 0.002793296089385475, 'interest': 0.002793296089385475, 'scientist': 0.002793296089385475, 'advertised': 0.002793296089385475, 'cutting': 0.002793296089385475, 'edge': 0.002793296089385475, 'contribute': 0.002793296089385475, 'help': 0.002793296089385475, 'advance': 0.002793296089385475, 'company': 0.002793296089385475, 's': 0.002793296089385475, 'mission': 0.002793296089385475, 'previous': 0.002793296089385475, 'engineer': 0.002793296089385475, 'had': 0.002793296089385475, 'privilege': 0.002793296089385475, 'senior': 0.002793296089385475, 'velopers': 0.002793296089385475, 'engineers': 0.002793296089385475, 'real': 0.002793296089385475, 'time': 0.002793296089385475, 'using': 0.002793296089385475, 'honed': 0.002793296089385475, 'ability': 0.002793296089385475, 'tackle': 0.002793296089385475, 'complex': 0.002793296089385475, 'challenges': 0.002793296089385475, 'deliver': 0.002793296089385475, 'scalable': 0.002793296089385475, 'meet': 0.002793296089385475, 'both': 0.002793296089385475, 'technical': 0.002793296089385475, 'busi': 0.002793296089385475, 'ness': 0.002793296089385475, 'objectives': 0.002793296089385475, 'proven': 0.002793296089385475, 'algorithms': 0.002793296089385475, 'analyze': 0.002793296089385475, 'interpret': 0.002793296089385475, 'generated': 0.002793296089385475, 'by': 0.002793296089385475, 'vices': 0.002793296089385475, 'additionally': 0.002793296089385475, 'managed': 0.002793296089385475, 'genera': 0.002793296089385475, 'tion': 0.002793296089385475, 'datasets': 0.002793296089385475, 'generation': 0.002793296089385475, 'evaluated': 0.002793296089385475, 'improved': 0.002793296089385475, 'existing': 0.002793296089385475, 'deployed': 0.002793296089385475, 'col': 0.002793296089385475, 'laborated': 0.002793296089385475, 'external': 0.002793296089385475, 'integrate': 0.002793296089385475, 'third': 0.002793296089385475, 'party': 0.002793296089385475, 'systems': 0.002793296089385475, 'proficiency': 0.002793296089385475, 'python': 0.002793296089385475, 'tensorflow': 0.002793296089385475, 'pytorch': 0.002793296089385475, 'opencv': 0.002793296089385475, 'relevant': 0.002793296089385475, 'equipped': 0.002793296089385475, 'necessary': 0.002793296089385475, 'skills': 0.002793296089385475, 'excel': 0.002793296089385475, 'adept': 0.002793296089385475, 'clean': 0.002793296089385475, 'well': 0.002793296089385475, 'documented': 0.002793296089385475, 'modular': 0.002793296089385475, 'code': 0.002793296089385475, 'experimenting': 0.002793296089385475, 'ar': 0.002793296089385475, 'chitectures': 0.002793296089385475, 'performing': 0.002793296089385475, 'hyper': 0.002793296089385475, 'parameter': 0.002793296089385475, 'tuning': 0.002793296089385475, 'conducting': 0.002793296089385475, 'transfer': 0.002793296089385475, 'optimize': 0.002793296089385475, 'performance': 0.002793296089385475, 'furthermore': 0.002793296089385475, 'generating': 0.002793296089385475, 'cross': 0.002793296089385475, 'functional': 0.002793296089385475, 'mentoring': 0.002793296089385475, 'junior': 0.002793296089385475, 'members': 0.002793296089385475, 'aligns': 0.002793296089385475, 'responsibilities': 0.002793296089385475, 'outlined': 0.002793296089385475, 'job': 0.002793296089385475, 'description': 0.002793296089385475, 'passionate': 0.002793296089385475, 'staying': 0.002793296089385475, 'abreast': 0.002793296089385475, 'latest': 0.002793296089385475, 'advancements': 0.002793296089385475, 'com': 0.002793296089385475, 'mitted': 0.002793296089385475, 'within': 0.002793296089385475, 'organization': 0.002793296089385475, 'particularly': 0.002793296089385475, 'leverage': 0.002793296089385475, 'ex': 0.002793296089385475, 'pertise': 0.002793296089385475, 'state': 0.002793296089385475, 'art': 0.002793296089385475, 'applications': 0.002793296089385475, 'object': 0.002793296089385475, 'detection': 0.002793296089385475, 'image': 0.002793296089385475, 'text': 0.002793296089385475, 'domains': 0.002793296089385475, 'confident': 0.002793296089385475, 'skill': 0.002793296089385475, 'set': 0.002793296089385475, 'make': 0.002793296089385475, 'candidate': 0.002793296089385475, 'eager': 0.002793296089385475, 'bring': 0.002793296089385475, 'unique': 0.002793296089385475, 'perspective': 0.002793296089385475, 'insights': 0.002793296089385475, 'warm': 0.002793296089385475, 'regards': 0.002793296089385475, 'sharfa': 0.002793296089385475, 'mallick': 0.002793296089385475}), Counter({'what': 0.3333333333333333, 'is': 0.3333333333333333, 'qualification': 0.3333333333333333})]\n",
            "IDF: {'me': 0.6931471805599453, 'junior': 0.6931471805599453, 'objectives': 0.6931471805599453, 'classification': 0.6931471805599453, 'skills': 0.6931471805599453, 'ar': 0.6931471805599453, 'cameras': 0.6931471805599453, 'additionally': 0.6931471805599453, 'experimenting': 0.6931471805599453, 'conducting': 0.6931471805599453, 'ex': 0.6931471805599453, 'mentoring': 0.6931471805599453, 'state': 0.6931471805599453, 'members': 0.6931471805599453, 'abreast': 0.6931471805599453, 'my': 0.6931471805599453, 'leverage': 0.6931471805599453, 'deliver': 0.6931471805599453, 'detection': 0.6931471805599453, 'had': 0.6931471805599453, 'opportunity': 0.6931471805599453, 'algorithms': 0.6931471805599453, 'well': 0.6931471805599453, 'as': 0.6931471805599453, 'candidate': 0.6931471805599453, 'mallick': 0.6931471805599453, 'regards': 0.6931471805599453, 'sharfa': 0.6931471805599453, 'machine': 0.6931471805599453, 'optimize': 0.6931471805599453, 'both': 0.6931471805599453, 'data': 0.6931471805599453, 'confident': 0.6931471805599453, 'furthermore': 0.6931471805599453, 'systems': 0.6931471805599453, 'col': 0.6931471805599453, 'pytorch': 0.6931471805599453, 'improved': 0.6931471805599453, 'challenges': 0.6931471805599453, 'your': 0.6931471805599453, 'time': 0.6931471805599453, 'perspective': 0.6931471805599453, 'engineer': 0.6931471805599453, 'edge': 0.6931471805599453, 'privilege': 0.6931471805599453, 'hiring': 0.6931471805599453, 'interpret': 0.6931471805599453, 'warm': 0.6931471805599453, 'honed': 0.6931471805599453, 'ability': 0.6931471805599453, 's': 0.6931471805599453, 'passionate': 0.6931471805599453, 'tuning': 0.6931471805599453, 'text': 0.6931471805599453, 'deployed': 0.6931471805599453, 'developing': 0.6931471805599453, 'managed': 0.6931471805599453, 'complex': 0.6931471805599453, 'learning': 0.6931471805599453, 'opencv': 0.6931471805599453, 'equipped': 0.6931471805599453, 'technologies': 0.6931471805599453, 'ness': 0.6931471805599453, 'using': 0.6931471805599453, 'technical': 0.6931471805599453, 'analyze': 0.6931471805599453, 'dear': 0.6931471805599453, 'latest': 0.6931471805599453, 'clean': 0.6931471805599453, 'bring': 0.6931471805599453, 'manager': 0.6931471805599453, 'has': 0.6931471805599453, 'team': 0.6931471805599453, 'previous': 0.6931471805599453, 'evaluated': 0.6931471805599453, 'pertise': 0.6931471805599453, 'role': 0.6931471805599453, 'engineers': 0.6931471805599453, 'background': 0.6931471805599453, 'help': 0.6931471805599453, 'position': 0.6931471805599453, 'art': 0.6931471805599453, 'devices': 0.6931471805599453, 'tion': 0.6931471805599453, 'strong': 0.6931471805599453, 'organization': 0.6931471805599453, 'model': 0.6931471805599453, 'at': 0.6931471805599453, 'transfer': 0.6931471805599453, 'ai': 0.6931471805599453, 'solutions': 0.6931471805599453, 'documented': 0.6931471805599453, 'meet': 0.6931471805599453, 'innovation': 0.6931471805599453, 'staying': 0.6931471805599453, 'insights': 0.6931471805599453, 'other': 0.6931471805599453, 'advance': 0.6931471805599453, 'teams': 0.6931471805599453, 'that': 0.6931471805599453, 'of': 0.6931471805599453, 'object': 0.6931471805599453, 'responsibilities': 0.6931471805599453, 'existing': 0.6931471805599453, 'modular': 0.6931471805599453, 'scalable': 0.6931471805599453, 'aligns': 0.6931471805599453, 'velopers': 0.6931471805599453, 'domains': 0.6931471805599453, 'track': 0.6931471805599453, 'models': 0.6931471805599453, 'third': 0.6931471805599453, 'mission': 0.6931471805599453, 'graphics': 0.6931471805599453, 'datasets': 0.6931471805599453, 'cutting': 0.6931471805599453, 'by': 0.6931471805599453, 'mitted': 0.6931471805599453, 'applications': 0.6931471805599453, 'generation': 0.6931471805599453, 'vision': 0.6931471805599453, 'external': 0.6931471805599453, 'tensorflow': 0.6931471805599453, 'image': 0.6931471805599453, 'description': 0.6931471805599453, 'com': 0.6931471805599453, 'skill': 0.6931471805599453, 'scientist': 0.6931471805599453, 'advertised': 0.6931471805599453, 'smart': 0.6931471805599453, 'de': 0.6931471805599453, 'this': 0.6931471805599453, 'vices': 0.6931471805599453, 'tackle': 0.6931471805599453, 'am': 0.6931471805599453, 'for': 0.6931471805599453, 'the': 0.6931471805599453, 'to': 0.6931471805599453, 'driving': 0.6931471805599453, 'functional': 0.6931471805599453, 'senior': 0.6931471805599453, 'and': 0.6931471805599453, 'make': 0.6931471805599453, 'advancements': 0.6931471805599453, 'python': 0.6931471805599453, 'outlined': 0.6931471805599453, 'cross': 0.6931471805599453, 'contribute': 0.6931471805599453, 'closely': 0.6931471805599453, 'unique': 0.6931471805599453, 'with': 0.6931471805599453, 'real': 0.6931471805599453, 'within': 0.6931471805599453, 'generating': 0.6931471805599453, 'excel': 0.6931471805599453, 'job': 0.6931471805599453, 'set': 0.6931471805599453, 'develop': 0.6931471805599453, 'company': 0.6931471805599453, 'record': 0.6931471805599453, 'code': 0.6931471805599453, 'in': 0.6931471805599453, 'writing': 0.6931471805599453, 'eager': 0.6931471805599453, 'genera': 0.6931471805599453, 'express': 0.6931471805599453, 'adept': 0.6931471805599453, 'a': 0.6931471805599453, 'collaborating': 0.6931471805599453, 'proven': 0.6931471805599453, 'performance': 0.6931471805599453, 'laborated': 0.6931471805599453, 'particularly': 0.6931471805599453, 'generated': 0.6931471805599453, 'i': 0.6931471805599453, 'excited': 0.6931471805599453, 'computer': 0.6931471805599453, 'necessary': 0.6931471805599453, 'experience': 0.6931471805599453, 'interest': 0.6931471805599453, 'integrate': 0.6931471805599453, 'chitectures': 0.6931471805599453, 'iot': 0.6931471805599453, 'parameter': 0.6931471805599453, 'about': 0.6931471805599453, 'party': 0.6931471805599453, 'proficiency': 0.6931471805599453, 'busi': 0.6931471805599453, 'relevant': 0.6931471805599453, 'hyper': 0.6931471805599453, 'have': 0.6931471805599453, 'performing': 0.6931471805599453, 'is': 0.6931471805599453, 'what': 0.6931471805599453, 'qualification': 0.6931471805599453}\n",
            "TF-IDF: [{'dear': 0.001936165308826663, 'hiring': 0.001936165308826663, 'manager': 0.001936165308826663, 'i': 0.02129781839709329, 'am': 0.015489322470613303, 'writing': 0.003872330617653326, 'to': 0.02710631432357328, 'express': 0.001936165308826663, 'my': 0.015489322470613303, 'interest': 0.001936165308826663, 'in': 0.017425487779439967, 'the': 0.02129781839709329, 'data': 0.003872330617653326, 'scientist': 0.001936165308826663, 'position': 0.003872330617653326, 'at': 0.003872330617653326, 'as': 0.003872330617653326, 'advertised': 0.001936165308826663, 'with': 0.015489322470613303, 'a': 0.009680826544133313, 'strong': 0.003872330617653326, 'background': 0.003872330617653326, 'developing': 0.0058084959264799885, 'cutting': 0.001936165308826663, 'edge': 0.001936165308826663, 'ai': 0.0058084959264799885, 'solutions': 0.0058084959264799885, 'and': 0.04840413272066657, 'track': 0.003872330617653326, 'record': 0.003872330617653326, 'of': 0.011616991852959977, 'driving': 0.003872330617653326, 'innovation': 0.003872330617653326, 'excited': 0.003872330617653326, 'about': 0.0058084959264799885, 'opportunity': 0.003872330617653326, 'contribute': 0.001936165308826663, 'your': 0.0058084959264799885, 'team': 0.0058084959264799885, 'help': 0.001936165308826663, 'advance': 0.001936165308826663, 'company': 0.001936165308826663, 's': 0.001936165308826663, 'mission': 0.001936165308826663, 'previous': 0.001936165308826663, 'role': 0.003872330617653326, 'computer': 0.003872330617653326, 'vision': 0.003872330617653326, 'machine': 0.011616991852959977, 'learning': 0.01355315716178664, 'engineer': 0.001936165308826663, 'had': 0.001936165308826663, 'privilege': 0.001936165308826663, 'collaborating': 0.003872330617653326, 'closely': 0.003872330617653326, 'senior': 0.001936165308826663, 'de': 0.003872330617653326, 'velopers': 0.001936165308826663, 'engineers': 0.001936165308826663, 'develop': 0.003872330617653326, 'real': 0.001936165308826663, 'time': 0.001936165308826663, 'using': 0.001936165308826663, 'iot': 0.0058084959264799885, 'devices': 0.003872330617653326, 'smart': 0.0058084959264799885, 'cameras': 0.0058084959264799885, 'this': 0.0058084959264799885, 'experience': 0.0058084959264799885, 'has': 0.003872330617653326, 'honed': 0.001936165308826663, 'ability': 0.001936165308826663, 'tackle': 0.001936165308826663, 'complex': 0.001936165308826663, 'challenges': 0.001936165308826663, 'deliver': 0.001936165308826663, 'scalable': 0.001936165308826663, 'that': 0.003872330617653326, 'meet': 0.001936165308826663, 'both': 0.001936165308826663, 'technical': 0.001936165308826663, 'busi': 0.001936165308826663, 'ness': 0.001936165308826663, 'objectives': 0.001936165308826663, 'have': 0.0058084959264799885, 'proven': 0.001936165308826663, 'models': 0.0058084959264799885, 'algorithms': 0.001936165308826663, 'analyze': 0.001936165308826663, 'interpret': 0.001936165308826663, 'generated': 0.001936165308826663, 'by': 0.001936165308826663, 'vices': 0.001936165308826663, 'additionally': 0.001936165308826663, 'managed': 0.001936165308826663, 'genera': 0.001936165308826663, 'tion': 0.001936165308826663, 'datasets': 0.001936165308826663, 'for': 0.007744661235306652, 'model': 0.0058084959264799885, 'generation': 0.001936165308826663, 'evaluated': 0.001936165308826663, 'improved': 0.001936165308826663, 'existing': 0.001936165308826663, 'deployed': 0.001936165308826663, 'col': 0.001936165308826663, 'laborated': 0.001936165308826663, 'external': 0.001936165308826663, 'teams': 0.003872330617653326, 'integrate': 0.001936165308826663, 'third': 0.001936165308826663, 'party': 0.001936165308826663, 'systems': 0.001936165308826663, 'proficiency': 0.001936165308826663, 'python': 0.001936165308826663, 'tensorflow': 0.001936165308826663, 'pytorch': 0.001936165308826663, 'opencv': 0.001936165308826663, 'other': 0.003872330617653326, 'relevant': 0.001936165308826663, 'technologies': 0.003872330617653326, 'equipped': 0.001936165308826663, 'me': 0.003872330617653326, 'necessary': 0.001936165308826663, 'skills': 0.001936165308826663, 'excel': 0.001936165308826663, 'adept': 0.001936165308826663, 'clean': 0.001936165308826663, 'well': 0.001936165308826663, 'documented': 0.001936165308826663, 'modular': 0.001936165308826663, 'code': 0.001936165308826663, 'experimenting': 0.001936165308826663, 'ar': 0.001936165308826663, 'chitectures': 0.001936165308826663, 'performing': 0.001936165308826663, 'hyper': 0.001936165308826663, 'parameter': 0.001936165308826663, 'tuning': 0.001936165308826663, 'conducting': 0.001936165308826663, 'transfer': 0.001936165308826663, 'optimize': 0.001936165308826663, 'performance': 0.001936165308826663, 'furthermore': 0.001936165308826663, 'generating': 0.001936165308826663, 'graphics': 0.003872330617653326, 'cross': 0.001936165308826663, 'functional': 0.001936165308826663, 'mentoring': 0.001936165308826663, 'junior': 0.001936165308826663, 'members': 0.001936165308826663, 'aligns': 0.001936165308826663, 'responsibilities': 0.001936165308826663, 'outlined': 0.001936165308826663, 'job': 0.001936165308826663, 'description': 0.001936165308826663, 'passionate': 0.001936165308826663, 'staying': 0.001936165308826663, 'abreast': 0.001936165308826663, 'latest': 0.001936165308826663, 'advancements': 0.001936165308826663, 'com': 0.001936165308826663, 'mitted': 0.001936165308826663, 'within': 0.001936165308826663, 'organization': 0.001936165308826663, 'particularly': 0.001936165308826663, 'leverage': 0.001936165308826663, 'ex': 0.001936165308826663, 'pertise': 0.001936165308826663, 'state': 0.001936165308826663, 'art': 0.001936165308826663, 'applications': 0.001936165308826663, 'object': 0.001936165308826663, 'detection': 0.001936165308826663, 'image': 0.001936165308826663, 'classification': 0.003872330617653326, 'text': 0.001936165308826663, 'domains': 0.001936165308826663, 'confident': 0.001936165308826663, 'skill': 0.001936165308826663, 'set': 0.001936165308826663, 'make': 0.001936165308826663, 'candidate': 0.001936165308826663, 'eager': 0.001936165308826663, 'bring': 0.001936165308826663, 'unique': 0.001936165308826663, 'perspective': 0.001936165308826663, 'insights': 0.001936165308826663, 'warm': 0.001936165308826663, 'regards': 0.001936165308826663, 'sharfa': 0.001936165308826663, 'mallick': 0.001936165308826663}, {'what': 0.23104906018664842, 'is': 0.23104906018664842, 'qualification': 0.23104906018664842}]\n",
            "Error: TF-IDF vectors are not properly computed.\n",
            "Error: TF-IDF vectors are not properly computed.\n",
            "Error: TF-IDF vectors are not properly computed.\n",
            "No relevant passages found.\n",
            "Answer: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hlLyXQGJblzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n6BIhUaubl27"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPKwj7JRGx5W+26LYBscyX+",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}